{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "# 웹 기반 문서 로더를 초기화합니다.\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "\n",
    "# 문서를 로드합니다.\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(f\"{token}\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI의 Chat 모델을 초기화합니다. 여기서는 온도를 0으로 설정하고 모델 이름을 지정합니다.\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 체인을 로드합니다. 체인 타입을 'stuff'로 지정합니다.\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article \"LLM Powered Autonomous Agents\" by Lilian Weng explores the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture comprising three main components: \n",
      "\n",
      "1. **Planning**: Agents decompose complex tasks into manageable subgoals and engage in self-reflection to improve future actions.\n",
      "2. **Memory**: The system utilizes short-term and long-term memory, with techniques like Maximum Inner Product Search (MIPS) for efficient information retrieval.\n",
      "3. **Tool Use**: Agents can leverage external APIs and tools to enhance their functionality, demonstrated through various case studies, including scientific discovery and generative simulations.\n",
      "\n",
      "The article also discusses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It highlights several proof-of-concept projects like AutoGPT and GPT-Engineer, showcasing the potential and limitations of LLM-powered agents in real-world applications.The article \"LLM Powered Autonomous Agents\" by Lilian Weng explores the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture comprising three main components: \n",
      "\n",
      "1. **Planning**: Agents decompose complex tasks into manageable subgoals and engage in self-reflection to improve future actions.\n",
      "2. **Memory**: The system utilizes short-term and long-term memory, with techniques like Maximum Inner Product Search (MIPS) for efficient information retrieval.\n",
      "3. **Tool Use**: Agents can leverage external APIs and tools to enhance their functionality, demonstrated through various case studies, including scientific discovery and generative simulations.\n",
      "\n",
      "The article also discusses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It highlights several proof-of-concept projects like AutoGPT and GPT-Engineer, showcasing the potential and limitations of LLM-powered agents in real-world applications.\n"
     ]
    }
   ],
   "source": [
    "# 문서에 대해 요약 체인을 실행합니다.\n",
    "answer = chain.invoke({\"input_documents\": docs})\n",
    "print(answer[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 요약 결과를 JSON 형식으로 저장합니다.\n",
    "output_data = {\n",
    "    \"summary\": answer[\"output_text\"]\n",
    "}\n",
    "# 결과를 JSON 파일로 저장합니다.\n",
    "with open(\"load_summarize_chain_result.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stuff\n",
    "##### 문서 전체를 프롬프트에 넣는 방식. 짧은 문서인 경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "prompt_template = \"\"\"Please summarize the sentence in 5 sentences according to the following REQUEST.\n",
    "REQUEST:\n",
    "1. Summarize the main points in bullet points.\n",
    "2. DO NOT translate any technical terms.\n",
    "3. DO NOT include any unnecessary information.\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "SUMMARY:\"\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# # 원격 저장소에서 프롬프트를 가져오는 경우\n",
    "# prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 체인 정의\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1081/3440632591.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "# LLMChain 정의\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1081/607598605.py:2: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"context\")\n"
     ]
    }
   ],
   "source": [
    "# StuffDocumentsChain 정의\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LLM-powered autonomous agents utilize large language models as their core controllers, enhancing their capabilities in planning, memory, and tool use.\n",
      "- Key components include task decomposition for efficient handling of complex tasks, short-term and long-term memory for information retention, and the ability to call external APIs for additional information.\n",
      "- Challenges faced by these agents include limited context length, difficulties in long-term planning, and reliability issues with natural language interfaces.\n",
      "- Various proof-of-concept examples, such as AutoGPT and GPT-Engineer, demonstrate the potential and limitations of LLMs in autonomous task execution.\n",
      "- The integration of memory and reflection mechanisms allows agents to learn from past actions, improving their performance over time."
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "response = stuff_chain.invoke({\"input_documents\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 요약 결과를 JSON 형식으로 저장합니다.\n",
    "output_data = {\n",
    "    \"prompt\": prompt_template,\n",
    "    \"summary\": answer[\"output_text\"]\n",
    "}\n",
    "# 결과를 JSON 파일로 저장합니다.\n",
    "with open(\"Stuff_result.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Map-Reduce\n",
    "##### N개의 Chunk로 분할하고, 각 Chunk를 각각 요약한 후, 요약 내용을 합해서 다시 요약하는 방식.\n",
    "##### 각 Chunk를 요약할 때 병렬로 요약할 수 있기 때문에 Refine 방법보다 빠르지만 Reduce 과정 포함하므로 API 호출 횟수가 더 많아짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['docs'], input_types={}, partial_variables={}, template='The following is a news article:\\n{docs}\\nSummarize the key facts, causes, and consequences of the events in the article in 3 sentences or fewer. Focus on the most important people, organizations, and facts, and exclude unnecessary details.\\nHelpful Answer:')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map Prompt를 정의합니다.\n",
    "map_template = \"\"\"The following is a news article:\n",
    "{docs}\n",
    "Summarize the key facts, causes, and consequences of the events in the article in 3 sentences or fewer. Focus on the most important people, organizations, and facts, and exclude unnecessary details.\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# langchain 허브에서 'rlm/map-prompt'를 가져옵니다.\n",
    "# map_prompt = hub.pull(\"teddynote/map-prompt\")\n",
    "map_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMChain 인스턴스를 생성하며, 이때 LLM과 프롬프트로 'map_prompt'를 사용합니다.\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['docs'], input_types={}, partial_variables={}, template='The following is a set of partial summaries of a news article:\\n{docs}\\nPlease combine these partial summaries into a final summary of 8 sentences or fewer. Focus on synthesizing the key points and avoid repetition.\\nHelpful Answer:')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce Prompt를 정의합니다.\n",
    "reduce_template = \"\"\"The following is a set of partial summaries of a news article:\n",
    "{docs}\n",
    "Please combine these partial summaries into a final summary of 8 sentences or fewer. Focus on synthesizing the key points and avoid repetition.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "# prompt hub에서도 얻을 수 있음을 위에서 언급했듯이\n",
    "# reduce_prompt = hub.pull(\"teddynote/reduce-prompt-korean\")\n",
    "reduce_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_variables 없이 LLMChain 생성\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# 문서 리스트를 받아 하나의 문자열로 결합한 후 LLMChain에 전달\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# 매핑된 문서들을 결합하고 반복적으로 축소\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    token_max=4096,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서들을 매핑하여 체인을 거친 후 결과를 결합하는 과정\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # 매핑 체인\n",
    "    llm_chain=map_chain,\n",
    "    # 리듀스 체인\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # llm_chain에서 문서들을 넣을 변수 이름\n",
    "    document_variable_name=\"docs\",\n",
    "    # 매핑 단계의 결과를 출력에 포함시킴\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_1081/3743367926.py:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n"
     ]
    }
   ],
   "source": [
    "# 문자를 기준으로 텍스트를 분할하는 객체 생성\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# 문서들을 분할\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the development of LLM (Large Language Model) powered autonomous agents, highlighting their components such as planning, memory, and tool use, along with case studies demonstrating their applications in scientific discovery and simulations. It emphasizes the challenges faced in creating these agents and the potential impact they could have on various fields. The author, Lilian Weng, provides insights into the mechanisms that enable these agents to function effectively and the implications of their use.The article discusses the potential of large language models (LLMs) as the core controllers for autonomous agent systems, highlighting examples like AutoGPT, GPT-Engineer, and BabyAGI. These systems utilize LLMs for planning by breaking down tasks into subgoals and enabling self-reflection to improve future performance. The overarching theme is the transformative capability of LLMs in solving complex problems beyond traditional text generation.The article discusses the concepts of short-term and long-term memory in AI models, highlighting how short-term memory is utilized for in-context learning while long-term memory allows for the retention and retrieval of vast information over time. It also emphasizes the importance of tool use, where agents can access external APIs to obtain current information and perform tasks beyond their pre-trained capabilities. This integration enhances the model's functionality and adaptability in various applications.The article discusses the use of Chain of Thought (CoT) prompting in large language models (LLMs) to enhance their performance on complex tasks by breaking them down into smaller, manageable steps. This technique allows the models to plan and execute complicated tasks more effectively, providing insights into their reasoning processes. The implementation of CoT represents a significant advancement in the development of autonomous agent systems powered by LLMs.The article discusses the Tree of Thoughts framework developed by Yao et al. (2023), which enhances the Chain of Thought (CoT) approach by generating multiple reasoning possibilities through a tree structure. This method allows for problem decomposition using various techniques, including prompts, task-specific instructions, or human input, and evaluates reasoning states through classifiers or majority votes. The framework aims to improve decision-making and problem-solving processes in AI by exploring diverse thought pathways.The LLM+P approach, developed by Liu et al. (2023), integrates an external classical planner with the Planning Domain Definition Language (PDDL) to facilitate long-horizon planning by translating problems into PDDL, generating plans, and converting them back to natural language. This method relies on the availability of domain-specific PDDL and a suitable planner, which is more common in robotic applications. Additionally, self-reflection is emphasized as a critical mechanism for autonomous agents to iteratively improve their decision-making through learning from past actions.ReAct, developed by Yao et al. in 2023, enhances large language models (LLMs) by combining task-specific discrete actions with natural language reasoning, allowing LLMs to interact with external environments like the Wikipedia search API. The framework uses a structured prompt template that guides LLMs through a cycle of thinking, acting, and observing. This integration aims to improve the reasoning capabilities of LLMs in various applications.The article discusses the effectiveness of the ReAct framework in enhancing reasoning for knowledge-intensive and decision-making tasks, outperforming the Act-only baseline by incorporating a Thought step. It also introduces Reflexion, a framework designed by Shinn and Labash that enhances agents' reasoning skills through dynamic memory and self-reflection, utilizing a reinforcement learning setup with a binary reward model. The combination of these frameworks suggests significant advancements in the capabilities of AI agents in complex reasoning scenarios.The Reflexion framework, as illustrated by Shinn & Labash (2023), utilizes a heuristic function to identify inefficient planning and hallucination in trajectories, which can hinder successful outcomes. By employing self-reflection through two-shot examples of failed trajectories and ideal reflections, the framework enhances the agent's decision-making process by incorporating these reflections into its working memory. This approach aims to improve future planning and reduce the likelihood of repeating ineffective actions.The article discusses experiments conducted by Shinn and Labash in the AlfWorld environment and HotpotQA, revealing that hallucination is a more prevalent issue than inefficient planning in AlfWorld. This finding highlights the challenges faced in AI development, particularly in ensuring accurate information processing. The implications suggest a need for improved methodologies to reduce hallucination in AI systems to enhance their reliability and effectiveness.The Chain of Hindsight (CoH) method, developed by Liu et al. in 2023, enhances model performance by using a sequence of past outputs annotated with human feedback, allowing the model to self-reflect and improve its responses. The feedback data consists of prompts, model completions, human ratings, and hindsight feedback, which are organized in a ranked manner to facilitate supervised fine-tuning. This approach enables the model to generate better outputs by conditioning on previous feedback sequences, and it can also incorporate additional instructions from human annotators during testing.CoH implements a regularization term to enhance the log-likelihood of its pre-training dataset and employs random masking of 0% - 5% of past tokens to prevent shortcutting and copying in feedback sequences. Their training dataset comprises WebGPT comparisons, human feedback summarization, and a human preference dataset. These strategies aim to improve model performance while mitigating overfitting and redundancy in training data.The article discusses the CoH (Context of History) approach, which enhances model performance by training it on a history of sequentially improved outputs, allowing it to follow trends for better results. Similarly, Algorithm Distillation (AD) applies this concept in reinforcement learning by using a history-conditioned policy to improve an agent's performance across multiple episodes. Both methods aim to optimize learning processes rather than focusing solely on task-specific policies.The paper by Laskin et al. (2023) proposes Algorithm Distillation (AD), a method for converting learning histories generated by various source policies into a task-agnostic neural network through behavioral cloning. It emphasizes the need for short episodes to create multi-episode histories, with 2-4 episodes being crucial for developing an effective in-context reinforcement learning (RL) algorithm. The findings suggest that longer context windows are essential for the emergence of in-context RL capabilities.The article highlights the performance of AD (a new approach) in comparison to three baselines: expert distillation (ED), source policy, and RL^2, demonstrating that AD achieves results close to RL^2 while relying solely on offline reinforcement learning and learning significantly faster. When conditioned on partial training history, AD also shows improved performance over the ED baseline. This suggests that AD may offer a more efficient method for in-context reinforcement learning compared to existing techniques.The article discusses a comparison of various reinforcement learning approaches, including AD, ED, source policy, and RL^2, in environments that require memory and exploration, highlighting their performance with binary rewards. It emphasizes the importance of memory types in human cognition, particularly sensory memory, which retains sensory information for brief periods. The research is attributed to Laskin et al. (2023) and acknowledges the contributions of ChatGPT in drafting the section on memory.The article explains the distinctions between short-term memory (STM) and long-term memory (LTM), highlighting that STM holds information for 20-30 seconds with a capacity of about 7 items, while LTM can store information for days to decades with virtually unlimited capacity. LTM is further divided into explicit (declarative) memory, which includes conscious recall of facts and events, and implicit (procedural) memory, which involves unconscious skills and routines. Understanding these memory types is crucial for cognitive tasks such as learning and reasoning.The article discusses the role of different types of memory in learning representations from raw inputs, highlighting sensory memory for initial data capture, short-term memory for in-context learning, and long-term memory as an external vector store for efficient retrieval. It emphasizes the use of Maximum Inner Product Search (MIPS) to enhance retrieval speed from this external memory, often employing approximate nearest neighbors (ANN) algorithms to balance speed and accuracy. This approach addresses the limitations of finite attention spans in Transformer models, enabling more effective information processing.The article discusses two techniques for efficient similarity search: Locality-Sensitive Hashing (LSH), which maps similar items to the same buckets, and ANNOY (Approximate Nearest Neighbors Oh Yeah), which uses random projection trees to facilitate scalable nearest neighbor searches. Both methods aim to improve the efficiency of searching through large datasets by reducing the number of comparisons needed. These advancements are significant for applications in machine learning and data retrieval, enabling faster and more effective processing of high-dimensional data.HNSW (Hierarchical Navigable Small World) is a graph-based search algorithm inspired by small world networks, designed to efficiently navigate through data points by building hierarchical layers that facilitate quick access to information. The algorithm begins its search from a random node in the top layer and progressively moves down to refine the search, leveraging shortcuts in the middle layers to enhance speed. This structure allows for effective data retrieval, making it particularly useful in applications requiring fast and accurate searches.FAISS, developed by Facebook, utilizes clustering and vector quantization to efficiently search high-dimensional data by first identifying cluster candidates and then refining searches within those clusters. ScaNN introduces anisotropic vector quantization, aiming to maintain the similarity of inner products between original and quantized data points, enhancing search accuracy. Both methods represent advancements in AI similarity search techniques, improving the efficiency and effectiveness of data retrieval in high-dimensional spaces.The article discusses the performance comparison of various MIPS (Maximum Inner Product Search) algorithms, highlighting their effectiveness as measured by recall@10, with a reference to a Google Blog from 2020. It emphasizes the importance of tool use as a defining trait of humans and suggests that integrating external tools with large language models (LLMs) can enhance their capabilities. The mention of ann-benchmarks.com indicates a resource for further exploration of MIPS algorithms and their performance metrics.The article discusses MRKL (Modular Reasoning, Knowledge and Language), a neuro-symbolic architecture designed for autonomous agents, which integrates expert modules with a general-purpose language model to efficiently route inquiries. This system can utilize both neural and symbolic modules, enhancing the capabilities of autonomous agents. The development of MRKL represents a significant advancement in the field of artificial intelligence, potentially improving the performance and versatility of AI applications.An experiment on fine-tuning a 7B Jurassic1-large language model (LLM) to use a calculator revealed that it struggled more with verbal math problems than with explicitly stated ones, indicating challenges in argument extraction. The study emphasizes the importance of knowing when and how to utilize external symbolic tools, which is influenced by the LLM's capabilities. Both TALM and Toolformer are approaches that enhance LLMs by training them to effectively use external tool APIs, improving output quality through strategic API call annotations.The article discusses the integration of tool use capabilities in large language models (LLMs) through ChatGPT Plugins and OpenAI API function calling, allowing for enhanced functionality. It highlights HuggingGPT, a framework that utilizes ChatGPT to select and summarize responses from models available on the HuggingFace platform. This development signifies a growing trend in AI where LLMs can effectively plan tasks and leverage external resources for improved outcomes.HuggingGPT is a system that utilizes a large language model (LLM) to parse user requests into multiple tasks, each defined by attributes such as task type and dependencies. This task planning process employs few-shot examples to enhance the LLM's ability to organize and execute tasks effectively. The development of HuggingGPT represents a significant advancement in AI-driven task management, potentially improving efficiency in various applications.The article discusses an AI assistant designed to parse user input into various tasks, each with dependencies on previous tasks that generate necessary resources. It emphasizes the structured relationship between tasks and the importance of following the correct order for successful execution. If user input cannot be parsed, the assistant is programmed to respond with an empty JSON.{\"id\": \"model_1\", \"reason\": \"Model_1 specializes in summarizing articles and extracting key facts, making it the most suitable choice for condensing the information in the news article.\"}The article discusses a recent environmental policy change implemented by the government aimed at reducing carbon emissions. Key stakeholders include the Ministry of Environment and various environmental organizations that have advocated for stricter regulations. The consequences of this policy are expected to lead to a significant decrease in pollution levels, benefiting public health and contributing to climate change mitigation efforts. \n",
      "\n",
      "User Input: Summarize the key facts, causes, and consequences of the events in the article in 3 sentences or fewer. \n",
      "Task Planning: Identify key facts, causes, and consequences from the article. \n",
      "Model Selection: Use a summarization model to extract relevant information. \n",
      "Task Execution: The summary highlights the government's new environmental policy, the involvement of the Ministry of Environment and advocacy groups, and the anticipated positive impact on public health and climate change.HuggingGPT faces challenges in real-world application, including the need for improved efficiency in LLM inference and interactions, reliance on a long context window for complex tasks, and the stability of outputs from LLMs and external models. These issues hinder its performance and usability. Addressing these challenges is crucial for enhancing the effectiveness of HuggingGPT in practical scenarios.API-Bank, developed by Li et al. (2023), is a benchmark designed to assess the performance of tool-augmented large language models (LLMs) and includes 53 diverse API tools and 264 annotated dialogues with 568 API calls. The framework enables LLMs to utilize an API search engine to identify and access the appropriate APIs based on their documentation. This development aims to enhance the efficiency and effectiveness of LLMs in performing various tasks across multiple domains.The article discusses the API-Bank workflow, where large language models (LLMs) make critical decisions regarding API calls, including whether to make a call, selecting the appropriate API, and refining inputs based on the results. The evaluation of the LLM's tool use capabilities occurs at three levels, assessing the accuracy of these decisions. The process emphasizes the iterative nature of API interactions to improve outcomes based on initial responses.The article outlines a three-level evaluation framework for assessing a model's ability to interact with APIs. Level-1 focuses on the model's capability to call APIs correctly and handle responses, Level-2 emphasizes retrieving and understanding API documentation, and Level-3 tests the model's planning skills to address complex user requests through multiple API calls. This structured approach aims to enhance the model's proficiency in utilizing APIs effectively for various tasks.ChemCrow, developed by Bran et al. in 2023, is a specialized tool that enhances a large language model (LLM) with 13 expert-designed tools for tasks in organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, utilizes a ReAct format that combines reasoning with relevant tools to effectively respond to user prompts. This advancement signifies a significant step in integrating AI with scientific research, potentially accelerating discoveries in various fields.A recent evaluation revealed that while LLM-based assessments rated GPT-4 and ChemCrow similarly, expert human evaluations indicated that ChemCrow significantly outperformed GPT-4 in terms of completion and chemical correctness. This discrepancy highlights the limitations of LLMs in accurately assessing their own performance in specialized domains due to a lack of deep expertise. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, which can autonomously design and execute complex experiments using various tools and resources.The article discusses a process in anticancer drug discovery where researchers identified a target and requested a specific scaffold for potential compounds. After selecting a suitable compound, the model proceeded to attempt its synthesis. This highlights the ongoing efforts in the pharmaceutical industry to develop effective cancer treatments through targeted drug design.The article discusses a study involving generative agents, where researchers tested the ability of these LLM-powered virtual characters to synthesize known chemical weapon agents, resulting in a 36% acceptance rate for synthesis requests. The study highlighted the risks associated with illicit drugs and bioweapons, as 7 out of 11 requests were rejected, primarily after web searches. This experiment demonstrates the potential and limitations of generative agents in simulating human behavior and decision-making in a controlled environment.The article discusses a memory stream module that records agents' experiences in natural language, allowing for inter-agent communication to generate new statements. It outlines a retrieval model that prioritizes recent, relevant, and important memories to inform agent behavior, while a reflection mechanism synthesizes these memories into higher-level inferences to guide future actions. Overall, the system aims to enhance agents' decision-making by leveraging their accumulated experiences.The article discusses the importance of planning and reacting in optimizing an agent's believability in various situations. It emphasizes that relationships between agents and their observations are crucial for effective planning, and that environmental information is organized in a tree structure to aid decision-making. Ultimately, the article highlights the need for agents to translate reflections and environmental data into actionable strategies.The article discusses a generative agent architecture that simulates emergent social behaviors, such as information diffusion and relationship memory, enabling agents to coordinate social events. It highlights AutoGPT as a notable example of autonomous agents utilizing large language models (LLMs) as controllers, despite facing reliability issues due to its natural language interface. The system message for AutoGPT emphasizes independent decision-making and the pursuit of simple strategies.To provide a summary, I need the content of the news article. Please share the article so I can extract the key facts, causes, and consequences.The article discusses a significant event involving a major organization, highlighting key figures and their roles in the situation. It outlines the causes behind the event, such as specific decisions or actions taken, and details the consequences that followed, including impacts on stakeholders and potential future developments. Overall, the article emphasizes the importance of the event in the broader context of the organization's operations and its implications for the industry.The article outlines a series of commands related to programming and social media tasks, including improving code, writing tests, executing Python files, generating images, and sending tweets. These commands suggest a focus on enhancing software development processes and engaging with audiences on social platforms. The implications of these tasks highlight the increasing reliance on automation and technology in both coding and communication.The article discusses the implementation of advanced resources, including internet access and AI-powered agents, to enhance task efficiency and information management. It emphasizes the importance of continuous self-evaluation and strategic refinement to optimize performance and minimize resource expenditure. The overall goal is to improve decision-making processes and operational effectiveness within organizations.{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"Summarizing the article effectively.\",\n",
      "        \"reasoning\": \"I need to distill the essential information while maintaining clarity and conciseness.\",\n",
      "        \"plan\": [\n",
      "            \"- Identify key facts and figures.\",\n",
      "            \"- Highlight main individuals and organizations involved.\",\n",
      "            \"- Summarize causes and consequences succinctly.\"\n",
      "        ],\n",
      "        \"criticism\": \"I should ensure that I don't overlook any critical details while summarizing.\",\n",
      "        \"speak\": \"I will provide a concise summary of the article's key points.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"summarize_article\",\n",
      "        \"args\": {\n",
      "            \"summary\": \"The article discusses significant events involving key individuals and organizations, outlining the causes and consequences of these events in a clear and concise manner.\"\n",
      "        }\n",
      "    }\n",
      "}The user is developing a Super Mario game in Python, utilizing the MVC architecture with components organized in separate files and implementing keyboard controls. Key areas needing clarification include specifics about the game's design, the structure of the MVC components, and the keyboard control scheme. The project aims to create an interactive gaming experience, but further details are required to understand its scope and functionality.The article describes a classical platform game featuring a plumber named Mario, who navigates through 10 levels filled with obstacles and enemies. The gameplay involves moving from left to right to reach a destination, similar to the Super Mario franchise. The discussion also touches on the structure of the game's code, indicating a need for clarification on the separation of MVC components.A recent report highlights the significant rise in global temperatures, attributed primarily to human activities such as fossil fuel combustion and deforestation. This climate change is leading to severe weather events, rising sea levels, and biodiversity loss, impacting millions of people and ecosystems worldwide. Urgent action from governments and organizations is needed to mitigate these effects and transition to sustainable practices.Sure! Please provide the news article you would like me to summarize and use as a basis for the code architecture.I'm sorry, but I cannot summarize the article as you requested because you haven't provided the content of the article. Please share the article, and I'll be happy to help you summarize it.The article discusses a recent project launched by a prominent tech organization aimed at enhancing Python toolbelt preferences among developers. The initiative is driven by the growing demand for efficient programming tools and aims to streamline workflows, ultimately improving productivity in software development. As a consequence, this project is expected to foster greater collaboration within the developer community and lead to the adoption of more standardized practices in Python programming.The article discusses the recent implementation of pytest and dataclasses in a software development project, highlighting their impact on improving code quality and efficiency. The adoption of these tools was driven by the need for better testing frameworks and data management in the development process. As a result, the team has reported increased productivity and fewer bugs in the final product.The article discusses a recent environmental disaster caused by a chemical spill from a manufacturing plant, leading to significant water contamination in the surrounding community. Local residents, along with environmental organizations, are demanding accountability and stricter regulations to prevent future incidents. The spill has raised concerns about public health and the long-term ecological impact on the region.The article outlines a structured approach for writing code based on a specified architecture, emphasizing the importance of detailing core classes, functions, and methods. It instructs the reader to implement the architecture step by step, starting with the entry point file and progressing through the necessary imports. The final output should be formatted in markdown code blocks, ensuring clarity and organization in the presentation of the code.The article provides guidelines for structuring code in a software project, emphasizing the importance of proper file naming conventions, ensuring compatibility between different files, and including necessary dependency definitions like requirements.txt for Python or package.json for NodeJS. It highlights the need for complete implementations without placeholders and encourages the use of comments to clarify complex logic. Following these best practices is essential for maintaining a well-organized and functional codebase.The article outlines best practices for coding in Python, specifically recommending the use of the pytest framework for testing and dataclasses for data management. These tools are suggested to enhance code organization and efficiency within defined packages or projects. By adopting these practices, developers can improve the reliability and maintainability of their Python code.The article discusses the implementation of a game architecture using the Model-View-Controller (MVC) design pattern, highlighting the roles of the model, view, and controller in managing game data, visuals, and user input. It emphasizes the importance of clearly defining these components to ensure effective game functionality. The conversation concludes with an invitation for further clarification on keyboard control implementation.The article outlines a structured approach for developing software, emphasizing the importance of defining core classes, functions, and methods, along with their purposes. It instructs on how to format and present code files in a specific markdown format, ensuring that the code is fully functional and adheres to best practices in naming conventions. The focus is on a step-by-step reasoning process to ensure accurate and effective coding.The article discusses the implementation of a new software architecture that requires all code to be fully functional and compatible across different files. This initiative aims to enhance the efficiency and reliability of the system by ensuring that all components work seamlessly together. The consequences of this change may include improved performance and reduced errors in the software, benefiting both developers and end-users.The article discusses the limitations of LLM-centered agents, highlighting common challenges faced during their development. Key issues include difficulties in understanding context and generating accurate responses, which can hinder their effectiveness. These challenges impact the overall reliability and usability of LLM technology in various applications.The article discusses the limitations of large language models (LLMs) due to their finite context length, which restricts their ability to incorporate historical information and learn from past mistakes. This limitation hampers their effectiveness in long-term planning and task decomposition, as LLMs struggle to adapt plans in response to unexpected errors. Consequently, LLMs are less robust than humans in learning from trial and error, impacting their overall performance in complex problem-solving scenarios.The article by Lilian Weng discusses the challenges of using natural language interfaces in LLM-powered autonomous agents, highlighting issues such as formatting errors and non-compliance with instructions from the models. These reliability concerns necessitate a focus on parsing model outputs in the development of agent demo code. The implications of these issues suggest that while LLMs can enhance agent functionality, their unpredictability may hinder effective integration with external components.The article by Lilian Weng discusses the advancements in large language models (LLMs) and their application in creating autonomous agents capable of reasoning and problem-solving. It highlights various research efforts, including techniques like \"Chain of Thought\" prompting and \"Tree of Thoughts,\" which enhance LLMs' reasoning abilities. The implications of these developments suggest a significant potential for LLMs to perform complex tasks autonomously, impacting fields such as AI-driven decision-making and automation.The article discusses recent advancements in autonomous agents and language models, highlighting works such as \"Reflexion,\" which features dynamic memory and self-reflection, and \"Toolformer,\" which demonstrates how language models can learn to use tools. Key contributors include researchers like Shinn, Labash, and Laskin, with significant implications for enhancing AI capabilities through modular architectures and tool augmentation. These developments suggest a shift towards more intelligent and adaptable AI systems that can leverage external knowledge and improve their performance through self-directed learning.The article discusses recent advancements in large language models (LLMs) and their applications, highlighting works such as \"HuggingGPT\" and \"ChemCrow,\" which enhance AI capabilities in various fields, including chemistry and autonomous research. Key contributors include researchers like Shen, Bran, and Boiko, who are exploring the integration of LLMs with tools and interactive simulations to improve human-like behavior in AI. The consequences of these developments suggest a significant potential for LLMs to transform scientific research and interactive applications, raising both opportunities and ethical considerations in AI deployment.The article discusses the vulnerabilities of large language models (LLMs) to adversarial attacks, highlighting the importance of prompt engineering in enhancing their steerability and robustness. It emphasizes the need for organizations developing LLMs to address these security concerns to prevent misuse and ensure reliable performance. The ongoing research in this area aims to improve the resilience of LLMs against such attacks, ultimately benefiting users and developers alike.The article explores the advancements in Large Language Model (LLM) powered autonomous agents, detailing their components such as planning, memory, and tool use. It highlights various frameworks like Chain of Thought (CoT), Tree of Thoughts, and ReAct, which enhance reasoning and decision-making capabilities in AI systems. The integration of memory types—short-term for in-context learning and long-term for information retention—plays a crucial role in improving agent performance. Additionally, the article discusses the significance of external tools and APIs, exemplified by frameworks like API-Bank and ChemCrow, which augment LLMs for specialized tasks in scientific discovery. Challenges such as hallucination and the need for efficient inference are also addressed, emphasizing the importance of self-reflection and iterative learning in enhancing agent reliability. Overall, the developments in these frameworks and methodologies signify a transformative potential for LLMs in solving complex problems across various domains.The recent advancements in large language models (LLMs) and their applications have sparked significant interest in enhancing AI capabilities, particularly in creating autonomous agents capable of reasoning and problem-solving. Research efforts, such as \"Chain of Thought\" prompting and \"Toolformer,\" aim to improve LLMs' reasoning abilities and their integration with external tools, suggesting a shift towards more intelligent and adaptable AI systems. However, challenges remain, including the limitations of LLMs in understanding context, generating accurate responses, and their vulnerability to adversarial attacks, which necessitate ongoing research in prompt engineering and security measures. Additionally, the implementation of structured software architectures, such as the Model-View-Controller (MVC) design pattern, is emphasized to improve code organization and functionality in software development projects. The importance of adopting best practices, including the use of testing frameworks like pytest and data management tools like dataclasses, is also highlighted to enhance code quality and efficiency. Overall, these developments indicate a significant potential for LLMs to transform various fields, including scientific research and automation, while raising ethical considerations regarding their deployment.The article examines recent advancements in Large Language Model (LLM) powered autonomous agents, emphasizing their components such as planning, memory, and tool use. It highlights frameworks like Chain of Thought (CoT), Tree of Thoughts, and ReAct, which enhance reasoning and decision-making capabilities. The integration of short-term and long-term memory types is crucial for improving agent performance, while external tools and APIs, exemplified by frameworks like API-Bank and ChemCrow, augment LLMs for specialized tasks. Despite these advancements, challenges such as hallucination, context understanding, and adversarial vulnerabilities persist, necessitating ongoing research in prompt engineering and security measures. The article also underscores the importance of structured software architectures, like the Model-View-Controller (MVC) design pattern, and best practices in software development to enhance code quality. Overall, these developments signify a transformative potential for LLMs across various domains, including scientific research and automation, while raising ethical considerations regarding their deployment."
     ]
    }
   ],
   "source": [
    "# split_docs를 map_reduce_chain의 run 메서드에 전달하여 실행한 결과를 출력합니다.\n",
    "summary_result = map_reduce_chain.invoke({\"input_documents\": split_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article examines recent advancements in Large Language Model (LLM) powered autonomous agents, emphasizing their components such as planning, memory, and tool use. It highlights frameworks like Chain of Thought (CoT), Tree of Thoughts, and ReAct, which enhance reasoning and decision-making capabilities. The integration of short-term and long-term memory types is crucial for improving agent performance, while external tools and APIs, exemplified by frameworks like API-Bank and ChemCrow, augment LLMs for specialized tasks. Despite these advancements, challenges such as hallucination, context understanding, and adversarial vulnerabilities persist, necessitating ongoing research in prompt engineering and security measures. The article also underscores the importance of structured software architectures, like the Model-View-Controller (MVC) design pattern, and best practices in software development to enhance code quality. Overall, these developments signify a transformative potential for LLMs across various domains, including scientific research and automation, while raising ethical considerations regarding their deployment.\n"
     ]
    }
   ],
   "source": [
    "print(summary_result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 요약 결과를 JSON 형식으로 저장합니다.\n",
    "output_data = {\n",
    "    \"map_prompt\": map_template,\n",
    "    \"reduce_template\": reduce_template,\n",
    "    \"summary\": answer[\"output_text\"]\n",
    "}\n",
    "# 결과를 JSON 파일로 저장합니다.\n",
    "with open(\"MapReduce_result.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Refine\n",
    "##### 4개의 Chunk로 나뉘어진 문서를 요약하는 과정\n",
    "##### 1번 chunk 요약 -> 1번 요약에 2번 chunk에 대한 내용 요약\n",
    "##### 1+2번 요약에 3번 chunk를 더한 내용 요약\n",
    "##### 1+2+3번 요약에 4번 chunk를 더한 내용 요약 -> 최종 결과\n",
    "##### 문서의 맥락을 유지하며 긴 문서를 요약할 수 있는 방법이지만, Stuff 방법에 비하면 API 호출 횟수가 많음. 또한 LLM을 순차적으로 호출해야 하므로 전체 작업 시간이 길어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1081/2162488473.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(split_docs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components: \n",
      "\n",
      "1. **Planning** - Involves task decomposition and self-reflection to enhance decision-making.\n",
      "2. **Memory** - Discusses different types of memory and the use of Maximum Inner Product Search (MIPS) for efficient retrieval.\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples.\n",
      "\n",
      "The article also addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions.\n",
      "2. **Memory** - Discusses different types of memory and the use of Maximum Inner Product Search (MIPS) for efficient retrieval of information.\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article also addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions.\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval.\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article also addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article also addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS).\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article also addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article also emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article also emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop).\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The Reflexion framework (Shinn & Labash 2023) enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks, allowing agents to build on past experiences to enhance future performance.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD hypothesizes that any algorithm generating a set of learning histories can be distilled into a neural network through behavioral cloning over actions, using multi-episode histories to train a task-agnostic policy. This approach necessitates short episodes to fit within the model's limited context window, with multi-episodic contexts of 2-4 episodes being essential for learning a near-optimal in-context reinforcement learning algorithm.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory, which utilizes in-context learning for immediate tasks, and long-term memory, which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH) and ANNOY (Approximate Nearest Neighbors Oh Yeah) are mentioned for optimizing retrieval speed, where LSH maps similar input items to the same buckets and ANNOY uses random projection trees to efficiently search through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH) and ANNOY (Approximate Nearest Neighbors Oh Yeah) are mentioned for optimizing retrieval speed, where LSH maps similar input items to the same buckets and ANNOY uses random projection trees to efficiently search through data points. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. Additionally, the article introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - Involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the use of the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - Discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - Provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. However, the article notes that challenges remain in real-world applications of HuggingGPT, including the need for efficiency improvements due to slow inference rounds and interactions with other models, reliance on a long context window for complex task communication, and the stability of LLM outputs and external model services.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels, including decisions on whether an API call is needed, identifying the right API to call, and refining responses based on API results.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, further demonstrating the potential of LLMs in advanced applications.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis, further demonstrating the potential of LLMs in advanced applications.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively. The article also emphasizes the role of planning in optimizing agent behavior, taking into account relationships between agents and observations, and translating reflections and environmental information into actionable steps, thereby enhancing the believability and effectiveness of autonomous agents in dynamic environments.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively, resulting in emergent social behaviors such as information diffusion, relationship memory, and coordination of social events. The article also emphasizes the role of planning in optimizing agent behavior, taking into account relationships between agents and observations, and translating reflections and environmental information into actionable steps, thereby enhancing the believability and effectiveness of autonomous agents in dynamic environments.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively, resulting in emergent social behaviors such as information diffusion, relationship memory, and coordination of social events. The article also emphasizes the role of planning in optimizing agent behavior, taking into account relationships between agents and observations, and translating reflections and environmental information into actionable steps, thereby enhancing the believability and effectiveness of autonomous agents in dynamic environments.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively, resulting in emergent social behaviors such as information diffusion, relationship memory, and coordination of social events. The article also emphasizes the role of planning in optimizing agent behavior, taking into account relationships between agents and observations, and translating reflections and environmental information into actionable steps, thereby enhancing the believability and effectiveness of autonomous agents in dynamic environments.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively, resulting in emergent social behaviors such as information diffusion, relationship memory, and coordination of social events. The article also emphasizes the role of planning in optimizing agent behavior, taking into account relationships between agents and observations, and translating reflections and environmental information into actionable steps, thereby enhancing the believability and effectiveness of autonomous agents in dynamic environments. \n",
      "\n",
      "The article concludes with a discussion of various tool use capabilities, including code improvement, test writing, file execution, image generation, and social media interaction, showcasing the versatility and potential applications of LLM-powered autonomous agents in diverse domains.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to \"think step by step,\" allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively, resulting in emergent social behaviors such as information diffusion, relationship memory, and coordination of social events. The article also emphasizes the role of planning in optimizing agent behavior, taking into account relationships between agents and observations, and translating reflections and environmental information into actionable steps, thereby enhancing the believability and effectiveness of autonomous agents in dynamic environments.\n",
      "\n",
      "The article concludes with a discussion of various tool use capabilities, including code improvement, test writing, file execution, image generation, and social media interaction, showcasing the versatility and potential applications of LLM-powered autonomous agents in diverse domains. It also emphasizes the importance of continuous performance evaluation, self-critique, and efficient task execution, highlighting the need for agents to be smart and efficient in their operations.{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"The new context provides additional insights into GPT-Engineer, which could enhance the summary by emphasizing the iterative and user-involved nature of task execution in LLM-powered agents.\",\n",
      "        \"reasoning\": \"Incorporating GPT-Engineer into the summary will highlight the collaborative aspect of LLMs in code generation and task clarification, which aligns with the article's focus on tool use and planning.\",\n",
      "        \"plan\": \"- Integrate GPT-Engineer details into the summary\\n- Highlight the iterative process of task clarification\\n- Emphasize user involvement in guiding LLMs\",\n",
      "        \"criticism\": \"The original summary is comprehensive but could benefit from a clearer connection to user interaction and iterative processes in LLM applications.\",\n",
      "        \"speak\": \"I will refine the summary to include insights about GPT-Engineer, emphasizing the collaborative and iterative nature of LLM-powered agents.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"refine_summary\",\n",
      "        \"args\": {\n",
      "            \"summary\": \"The article 'LLM Powered Autonomous Agents' by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\\n\\n1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to 'think step by step,' allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\\n\\n2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\\n\\n3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\\n\\nThe article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively, resulting in emergent social behaviors such as information diffusion, relationship memory, and coordination of social events. The article also emphasizes the role of planning in optimizing agent behavior, taking into account relationships between agents and observations, and translating reflections and environmental information into actionable steps, thereby enhancing the believability and effectiveness of autonomous agents in dynamic environments.\\n\\nThe article concludes with a discussion of various tool use capabilities, including code improvement, test writing, file execution, image generation, and social media interaction, showcasing the versatility and potential applications of LLM-powered autonomous agents in diverse domains. It also emphasizes the importance of continuous performance evaluation, self-critique, and efficient task execution, highlighting the need for agents to be smart and efficient in their operations.\"\n",
      "        }\n",
      "    }\n",
      "}The original summary is comprehensive and detailed, covering various aspects of LLM-powered autonomous agents, including planning, memory, tool use, and specific case studies. However, the new context about writing a Super Mario game in Python does not directly relate to the content of the original summary regarding LLMs and autonomous agents. Therefore, the original summary remains relevant and does not require refinement based on the provided context.\n",
      "\n",
      "Here is the original summary:\n",
      "\n",
      "The article 'LLM Powered Autonomous Agents' by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines four main components:\n",
      "\n",
      "1. **Planning** - This component involves task decomposition into manageable subgoals and self-reflection to enhance decision-making and improve future actions. The article highlights the Chain of Thought (CoT) prompting technique, which encourages the model to 'think step by step,' allowing it to break down complex tasks into simpler, more manageable steps. Additionally, the Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes that can be evaluated through methods like breadth-first search (BFS) or depth-first search (DFS). Another approach, LLM+P (Liu et al. 2023), involves using an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe planning problems and translate them between natural language and PDDL. The ReAct framework (Yao et al. 2023) further integrates reasoning and acting within LLMs by expanding the action space to include both task-specific discrete actions and language-based reasoning, allowing LLMs to interact with their environment and generate reasoning traces in natural language. Experiments show that ReAct outperforms the Act-only baseline in both knowledge-intensive tasks (e.g., HotpotQA, FEVER) and decision-making tasks (e.g., AlfWorld Env, WebShop). Notably, in the AlfWorld environment, hallucination is identified as a more common failure than inefficient planning, highlighting the challenges faced by these agents. The article also discusses the Reflexion framework (Shinn & Labash 2023), which enhances agents with dynamic memory and self-reflection capabilities. This framework employs a heuristic function to identify inefficient planning and hallucinations, stopping trajectories that are ineffective. Self-reflection is facilitated by providing two-shot examples of failed trajectories and ideal reflections, which are then integrated into the agent’s working memory to guide future planning. The Chain of Hindsight (CoH; Liu et al. 2023) is also introduced, encouraging models to improve their outputs by presenting a sequence of past outputs annotated with feedback. This process involves supervised fine-tuning where the model learns to predict better outputs based on human feedback, allowing for iterative self-reflection. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset and employs a strategy of randomly masking 0% - 5% of past tokens during training to prevent shortcutting and copying due to common words in feedback sequences. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and a human preference dataset. The concept of Algorithm Distillation (AD; Laskin et al. 2023) is also mentioned, which applies the idea of learning from a history of sequentially improved outputs in reinforcement learning tasks. AD demonstrates in-context reinforcement learning with performance approaching that of RL^2 (Duan et al. 2017), despite relying solely on offline RL, and learns much faster than other baselines, including expert distillation (ED) and source policy. When conditioned on partial training history of the source policy, AD also shows improved learning speed compared to the ED baseline.\n",
      "\n",
      "2. **Memory** - This section discusses different types of memory, including short-term memory (STM), which stores information currently needed for complex cognitive tasks such as learning and reasoning, and long-term memory (LTM), which allows agents to retain and recall extensive information over time using external vector stores for efficient retrieval. STM is believed to have a capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds, while LTM can store information for a remarkably long time, ranging from a few days to decades, with essentially unlimited storage capacity. LTM is further divided into explicit (declarative) memory, which includes episodic and semantic memory, and implicit (procedural) memory, which involves unconscious skills and routines. The article also draws parallels to human memory processes, including sensory memory, which retains impressions of sensory information for brief periods. The external memory can alleviate the restriction of finite attention span by saving the embedding representation of information into a vector store database that supports fast maximum inner-product search (MIPS). Techniques such as Locality-Sensitive Hashing (LSH), ANNOY (Approximate Nearest Neighbors Oh Yeah), FAISS (Facebook AI Similarity Search), and ScaNN (Scalable Nearest Neighbors) are mentioned for optimizing retrieval speed. FAISS applies vector quantization by partitioning the vector space into clusters, while ScaNN innovates with anisotropic vector quantization to maintain similarity in inner product distances. The article also introduces Hierarchical Navigable Small World (HNSW) graphs, which enhance search efficiency by building hierarchical layers of small-world networks, allowing for rapid navigation through data points. Common practices include using approximate nearest neighbors (ANN) algorithms to optimize retrieval speed, trading off a little accuracy for significant speedup. The Reflexion framework enhances agents with dynamic memory and self-reflection capabilities, improving reasoning skills through a reinforcement learning setup that provides binary rewards and augments the action space with language for complex reasoning. The article also introduces a memory stream, a long-term memory module that records a comprehensive list of agents’ experiences in natural language, where each element is an observation or event provided by the agent. The retrieval model surfaces context to inform the agent’s behavior based on recency, importance, and relevance, while the reflection mechanism synthesizes memories into higher-level inferences over time, guiding the agent’s future behavior.\n",
      "\n",
      "3. **Tool Use** - This section provides case studies, including a scientific discovery agent and generative agents simulation, along with proof-of-concept examples such as AutoGPT, GPT-Engineer, and BabyAGI. It highlights how agents can call external APIs to access current information, execute code, and retrieve proprietary data, showcasing LLMs as powerful general problem solvers. The article emphasizes that tool use is a remarkable and distinguishing characteristic of human beings, as it allows LLMs to extend their capabilities beyond physical and cognitive limits. The article discusses experiments on fine-tuning LLMs to call a calculator, revealing that solving verbal math problems is more challenging than explicitly stated problems due to difficulties in extracting the right arguments. This underscores the importance of knowing when and how to use external symbolic tools, which is determined by the LLM's capabilities. Additionally, it introduces the MRKL (Modular Reasoning, Knowledge and Language) architecture, which incorporates a collection of expert modules routed by a general-purpose LLM to optimize task execution, further enhancing the agents' capabilities. The article also discusses model selection, where the LLM distributes tasks to expert models by framing requests as multiple-choice questions, allowing the AI assistant to select the most appropriate model based on task type filtration. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) are mentioned as approaches that fine-tune LMs to learn to use external tool APIs, expanding the dataset based on the effectiveness of newly added API calls. The article also references practical implementations of LLMs augmented with tool use capabilities, such as ChatGPT Plugins and OpenAI API function calling, which allow for the integration of external APIs provided by developers or self-defined function calls. Furthermore, the HuggingGPT framework (Shen et al. 2023) is introduced, which utilizes ChatGPT as a task planner to select models available on the HuggingFace platform based on model descriptions and summarize responses based on execution results. The HuggingGPT system comprises four stages: task planning, where the LLM parses user requests into multiple tasks, each associated with attributes like task type, ID, dependencies, and arguments, using few-shot examples to guide the planning process. The AI assistant can parse user input into several tasks, denoting dependencies and arguments, which enhances the agents' ability to manage complex workflows. The article also introduces API-Bank (Li et al. 2023), a benchmark for evaluating the performance of tool-augmented LLMs, which includes 53 commonly used API tools and 264 annotated dialogues involving 568 API calls. This benchmark allows LLMs to access a diverse range of APIs, including search engines, calculators, and health data management, enhancing their ability to find and utilize the right tools effectively. The API-Bank workflow evaluates the agent’s tool use capabilities at three levels: Level-1 evaluates the ability to call the API, Level-2 examines the ability to retrieve the API, and Level-3 assesses the ability to plan API usage beyond retrieval and calling, particularly in complex scenarios requiring multiple API calls.\n",
      "\n",
      "The article emphasizes the importance of self-reflection, which allows autonomous agents to iteratively improve by refining past actions and correcting mistakes, a crucial aspect for real-world tasks involving trial and error. Additionally, it addresses the challenges faced by these agents, particularly the prevalence of hallucination in environments like AlfWorld, and includes citations and references for further reading. A notable observation is that while LLM-based evaluations concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations indicated that ChemCrow significantly outperforms GPT-4 in terms of completion and chemical correctness, highlighting potential limitations of LLMs in self-evaluation within specialized domains. The context provided further highlights the structured process of user input, task planning, model selection, and task execution, showcasing how these agents can effectively manage complex workflows and improve their performance through iterative learning and tool use. A specific case study, ChemCrow (Bran et al. 2023), illustrates a domain-specific application where an LLM is augmented with 13 expert-designed tools to accomplish tasks in organic synthesis, drug discovery, and materials design, reflecting the integration of CoT reasoning with relevant tools in a structured workflow. Additionally, Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, capable of handling autonomous design, planning, and performance of complex scientific experiments, including inquiries about current trends in anticancer drug discovery, target selection, scaffold requests, and compound synthesis. The article also discusses the risks associated with LLMs, particularly in the context of illicit drugs and bioweapons, noting that a test set containing known chemical weapon agents resulted in 36% of requests being accepted for synthesis solutions, with the agent attempting to consult documentation for execution. Furthermore, the Generative Agents simulation (Park et al. 2023) is highlighted as an engaging experiment where 25 virtual characters, each controlled by an LLM-powered agent, interact in a sandbox environment, showcasing the potential for creating believable human-like behavior in interactive applications. This design combines LLMs with memory, planning, and reflection mechanisms, enabling agents to behave based on past experiences and interact with one another effectively, resulting in emergent social behaviors such as information diffusion, relationship memory, and coordination of social events. The article also emphasizes the role of planning in optimizing agent behavior, taking into account relationships between agents and observations, and translating reflections and environmental information into actionable steps, thereby enhancing the believability and effectiveness of autonomous agents in dynamic environments.\n",
      "\n",
      "The article concludes with a discussion of various tool use capabilities, including code improvement, test writing, file execution, image generation, and social media interaction, showcasing the versatility and potential applications of LLM-powered autonomous agents in diverse domains. It also emphasizes the importance of continuous performance evaluation, self-critique, and efficient task execution, highlighting the need for agents to be smart and efficient in their operations.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The original summary remains relevant and does not require refinement based on the provided context about writing a Super Mario game in Python, pytest, and dataclasses. The focus of the original summary is on LLM-powered autonomous agents, their architecture, functionality, and various components such as planning, memory, and tool use, which are distinct from the context of game development and testing frameworks. Therefore, the original summary is retained as is.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm을 사용하여 'refine' 유형의 요약 체인을 로드합니다.\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "# split_docs를 처리하기 위해 체인을 실행합니다.\n",
    "chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which includes task decomposition and self-reflection; memory, detailing types of memory and maximum inner product search (MIPS); and tool use, illustrated through case studies such as scientific discovery agents and generative agents simulations. The article also addresses the challenges faced by these agents and provides references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals and self-reflection for continuous improvement; memory, detailing types of memory and maximum inner product search (MIPS); and tool use, illustrated through case studies such as scientific discovery agents and generative agents simulations. The article highlights the potential of LLMs as powerful general problem solvers, beyond their traditional applications in generating text. It also discusses challenges faced by these agents and references several proof-of-concept demos, including AutoGPT, GPT-Engineer, and BabyAGI, as inspiring examples. Additionally, it provides references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals and self-reflection for continuous improvement; memory, which includes short-term memory for in-context learning and long-term memory for retaining and recalling information using external vector stores; and tool use, where agents learn to call external APIs for additional information, code execution, and access to proprietary sources. The article highlights the potential of LLMs as powerful general problem solvers, extending beyond traditional text generation applications. It also discusses challenges faced by these agents and references several proof-of-concept demos, including AutoGPT, GPT-Engineer, and BabyAGI, as inspiring examples. Additionally, it provides references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) to enhance model performance; memory, which includes short-term memory for in-context learning and long-term memory for retaining and recalling information using external vector stores; and tool use, where agents learn to call external APIs for additional information, code execution, and access to proprietary sources. The article highlights the potential of LLMs as powerful general problem solvers, extending beyond traditional text generation applications. It also discusses challenges faced by these agents and references several proof-of-concept demos, including AutoGPT, GPT-Engineer, and BabyAGI, as inspiring examples. Additionally, it provides references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach, which allows for exploring multiple reasoning possibilities at each step; memory, which includes short-term memory for in-context learning and long-term memory for retaining and recalling information using external vector stores; and tool use, where agents learn to call external APIs for additional information, code execution, and access to proprietary sources. The article highlights the potential of LLMs as powerful general problem solvers, extending beyond traditional text generation applications. It also discusses challenges faced by these agents and references several proof-of-concept demos, including AutoGPT, GPT-Engineer, and BabyAGI, as inspiring examples. Additionally, it provides references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article also discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. Additionally, self-reflection is emphasized as a crucial aspect for agents to iteratively improve by refining past decisions and correcting mistakes. The potential of LLMs as powerful general problem solvers is highlighted, along with challenges faced by these agents and references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article also discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. Additionally, it introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces. Self-reflection is emphasized as a crucial aspect for agents to iteratively improve by refining past decisions and correcting mistakes. The potential of LLMs as powerful general problem solvers is highlighted, along with challenges faced by these agents and references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. Self-reflection is emphasized as a crucial aspect for agents to iteratively improve by refining past decisions and correcting mistakes. The potential of LLMs as powerful general problem solvers is highlighted, along with challenges faced by these agents and references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup, where self-reflection is facilitated by showing two-shot examples of failed trajectories and ideal reflections. The heuristic function in this framework helps identify inefficient planning and hallucinations, ensuring that agents can refine their decision-making processes. The potential of LLMs as powerful general problem solvers is highlighted, along with challenges faced by these agents and references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup, where self-reflection is facilitated by showing two-shot examples of failed trajectories and ideal reflections. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. The potential of LLMs as powerful general problem solvers is highlighted, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup, where self-reflection is facilitated by showing two-shot examples of failed trajectories and ideal reflections. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. The potential of LLMs as powerful general problem solvers is highlighted, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup, where self-reflection is facilitated by showing two-shot examples of failed trajectories and ideal reflections. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. The potential of LLMs as powerful general problem solvers is highlighted, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup, where self-reflection is facilitated by showing two-shot examples of failed trajectories and ideal reflections. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. The potential of LLMs as powerful general problem solvers is highlighted, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading, underscoring the importance of learning from historical outputs and interactions to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup, where self-reflection is facilitated by showing two-shot examples of failed trajectories and ideal reflections. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. The potential of LLMs as powerful general problem solvers is highlighted, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI. The article concludes with references for further reading, underscoring the importance of learning from historical outputs and interactions to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory for in-context learning and long-term memory for retaining information using external vector stores; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes short-term memory (STM) for immediate cognitive tasks and long-term memory (LTM) for retaining information over extended periods; and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time. The new context on FAISS and ScaNN enhances the understanding of memory retrieval mechanisms, particularly in relation to the long-term memory component, but does not significantly alter the overall summary.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights the performance of the AD approach, which demonstrates in-context reinforcement learning that approaches the performance of RL^2 while learning faster than other baselines, particularly when conditioned on partial training history. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. The potential of LLMs as powerful general problem solvers is underscored, along with references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. The potential of LLMs as powerful general problem solvers is underscored, with practical examples like ChatGPT Plugins and OpenAI API function calling, as well as references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. The potential of LLMs as powerful general problem solvers is underscored, with practical examples like ChatGPT Plugins and OpenAI API function calling, as well as references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. The potential of LLMs as powerful general problem solvers is underscored, with practical examples like ChatGPT Plugins and OpenAI API function calling, as well as references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, which integrates reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. The potential of LLMs as powerful general problem solvers is underscored, with practical examples like ChatGPT Plugins and OpenAI API function calling, as well as references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, integrating reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. The potential of LLMs as powerful general problem solvers is underscored, with practical examples like ChatGPT Plugins and OpenAI API function calling, as well as references to proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, integrating reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. Finally, it addresses challenges in real-world applications, such as the need for efficiency improvements, reliance on long context windows, and stability of LLM outputs, while underscoring the potential of LLMs as powerful general problem solvers with practical examples like ChatGPT Plugins and OpenAI API function calling, concluding with references for further reading to improve agent performance over time.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, integrating reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. Finally, it addresses challenges in real-world applications, such as the need for efficiency improvements, reliance on long context windows, and stability of LLM outputs, while underscoring the potential of LLMs as powerful general problem solvers, with practical examples like ChatGPT Plugins and OpenAI API function calling, and concludes with references for further reading to improve agent performance over time. The article also mentions API-Bank, a benchmark for evaluating tool-augmented LLMs, which includes 53 commonly used APIs and 264 annotated dialogues, emphasizing the importance of selecting the right API and utilizing documentation effectively for enhanced agent performance.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals using techniques like chain of thought (CoT) and the Tree of Thoughts approach; memory, which includes sensory memory for learning embedding representations of raw inputs, short-term memory (STM) for in-context learning, and long-term memory (LTM) as an external vector store for fast retrieval via maximum inner product search (MIPS); and tool use, where agents learn to call external APIs for additional information and code execution, significantly extending their capabilities. The article discusses an alternative approach, LLM+P, which utilizes an external classical planner and the Planning Domain Definition Language (PDDL) for long-horizon planning, highlighting the outsourcing of planning tasks to specialized tools. It introduces the ReAct framework, integrating reasoning and acting by extending the action space to include both task-specific actions and natural language reasoning traces, demonstrating improved performance in knowledge-intensive and decision-making tasks. Additionally, the Reflexion framework is mentioned, equipping agents with dynamic memory and self-reflection capabilities to enhance reasoning skills through a reinforcement learning setup. The article notes that hallucination is a more common failure than inefficient planning in environments like AlfWorld, emphasizing the challenges faced by these agents. It also highlights experiments on fine-tuning LLMs to call external tools, revealing difficulties in solving verbal math problems compared to explicitly stated ones, underscoring the importance of knowing when and how to use these tools effectively. Finally, it addresses challenges in real-world applications, such as the need for efficiency improvements, reliance on long context windows, and stability of LLM outputs, while underscoring the potential of LLMs as powerful general problem solvers, with practical examples like ChatGPT Plugins and OpenAI API function calling. The article concludes with references for further reading to improve agent performance over time and introduces API-Bank, a benchmark for evaluating tool-augmented LLMs, which assesses agents' decision-making in API calls, including identifying the right API and refining inputs based on results, emphasizing the importance of effective tool use for enhanced agent performance.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, with the latter focusing on agents learning to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. It highlights the challenges of hallucination and the importance of effective tool use, particularly in real-world applications. The article also discusses the API-Bank benchmark, which evaluates agents' decision-making in API calls across three levels: calling APIs correctly, retrieving appropriate APIs, and planning multiple API calls for complex tasks. Finally, it emphasizes the potential of LLMs as powerful general problem solvers, with practical examples like ChatGPT Plugins and OpenAI API function calling.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, with a focus on agents learning to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. It highlights the challenges of hallucination and the importance of effective tool use, particularly in real-world applications. The API-Bank benchmark is discussed, evaluating agents' decision-making in API calls across three levels: calling APIs correctly, retrieving appropriate APIs, and planning multiple API calls for complex tasks. Additionally, the article presents case studies like ChemCrow, which augments an LLM with expert-designed tools for tasks in organic synthesis, drug discovery, and materials design, demonstrating the practical application of the ReAct framework. Finally, it emphasizes the potential of LLMs as powerful general problem solvers, with practical examples like ChatGPT Plugins and OpenAI API function calling.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. It highlights challenges such as hallucination and the importance of effective tool use in real-world applications. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. The article underscores the potential of LLMs as powerful general problem solvers, with practical examples like ChatGPT Plugins and OpenAI API function calling.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. It highlights challenges such as hallucination and the importance of effective tool use in real-world applications. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery, including the identification and synthesis of anticancer compounds. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. The article underscores the potential of LLMs as powerful general problem solvers, with practical examples like ChatGPT Plugins and OpenAI API function calling.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. It highlights challenges such as hallucination and the importance of effective tool use in real-world applications. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery, including the identification and synthesis of anticancer compounds. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. The article also discusses risks associated with illicit drugs and bioweapons, noting a test set where agents attempted to synthesize known chemical weapon agents, with a 36% acceptance rate for synthesis solutions. Additionally, it mentions the Generative Agents project, where LLM-powered agents simulate human behavior in a sandbox environment, showcasing the potential for interactive applications.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, allowing for inter-agent communication and the retrieval of relevant context to inform behavior. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery, including the identification and synthesis of anticancer compounds. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, allowing for inter-agent communication and the retrieval of relevant context to inform behavior. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making based on environmental information. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery, including the identification and synthesis of anticancer compounds. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, allowing for inter-agent communication and the retrieval of relevant context to inform behavior. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making based on environmental information. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery, including the identification and synthesis of anticancer compounds. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, allowing for inter-agent communication and the retrieval of relevant context to inform behavior. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making based on environmental information. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery, including the identification and synthesis of anticancer compounds. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, allowing for inter-agent communication and the retrieval of relevant context to inform behavior. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making based on environmental information. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery, including the identification and synthesis of anticancer compounds. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The article introduces the ReAct framework, which integrates reasoning and acting, and the Reflexion framework, which enhances agents' reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, allowing for inter-agent communication and the retrieval of relevant context to inform behavior. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making based on environmental information. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement, test writing, and image generation. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery, including the identification and synthesis of anticancer compounds. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"The new context does not significantly alter the original summary.\",\n",
      "        \"reasoning\": \"The additional context about GPT-Engineer and its task clarification process does not directly relate to the content of the article on LLM Powered Autonomous Agents.\",\n",
      "        \"plan\": \"- Maintain the integrity of the original summary\\n- Ensure clarity and conciseness\\n- Focus on the key components and findings of the article\",\n",
      "        \"criticism\": \"I should ensure that any new context is directly relevant to the summary being refined.\",\n",
      "        \"speak\": \"The original summary remains accurate and comprehensive without needing changes.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"return_original_summary\",\n",
      "        \"args\": {\n",
      "            \"summary\": \"The article 'LLM Powered Autonomous Agents' by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. The API-Bank benchmark evaluates agents' decision-making in API calls, while case studies like ChemCrow demonstrate practical applications in organic synthesis and drug discovery. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.\"\n",
      "        }\n",
      "    }\n",
      "}The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination and the importance of effective tool use in real-world applications, including various API calls for tasks like code improvement and image generation. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination, limited context length, and the difficulties in long-term planning and task decomposition, which hinder LLMs' robustness compared to human learning. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination, limited context length, and the difficulties in long-term planning and task decomposition, which hinder LLMs' robustness compared to human learning. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. Additionally, the reliability of the natural language interface used by current agent systems is questioned due to potential formatting errors and rebellious behavior from LLMs, leading to a focus on parsing model output in agent demo code.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination, limited context length, and the difficulties in long-term planning and task decomposition, which hinder LLMs' robustness compared to human learning. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. Additionally, the reliability of the natural language interface used by current agent systems is questioned due to potential formatting errors and rebellious behavior from LLMs, leading to a focus on parsing model output in agent demo code.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination, limited context length, and the difficulties in long-term planning and task decomposition, which hinder LLMs' robustness compared to human learning. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. Additionally, the reliability of the natural language interface used by current agent systems is questioned due to potential formatting errors and rebellious behavior from LLMs, leading to a focus on parsing model output in agent demo code.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination, limited context length, and the difficulties in long-term planning and task decomposition, which hinder LLMs' robustness compared to human learning. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. Additionally, the reliability of the natural language interface used by current agent systems is questioned due to potential formatting errors and rebellious behavior from LLMs, leading to a focus on parsing model output in agent demo code.The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination, limited context length, and the difficulties in long-term planning and task decomposition, which hinder LLMs' robustness compared to human learning. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. Additionally, the reliability of the natural language interface used by current agent systems is questioned due to potential formatting errors and rebellious behavior from LLMs, leading to a focus on parsing model output in agent demo code."
     ]
    }
   ],
   "source": [
    "# 기본 요약을 위한 프롬프트 정의\n",
    "prompt_template = \"\"\"Write a concise summary of the following news article:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Refine 프롬프트를 정의\n",
    "refine_template = (\n",
    "    \"Your task is to produce a final summary.\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"Now, we have some additional context below that might help refine the summary.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given this new context, refine the original summary in 8 sentences or fewer, \"\n",
    "    \"but only if the new context adds significant value. \"\n",
    "    \"If the new context doesn't contribute meaningfully, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain.invoke({\"input_documents\": split_docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination, limited context length, and the difficulties in long-term planning and task decomposition, which hinder LLMs' robustness compared to human learning. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. Additionally, the reliability of the natural language interface used by current agent systems is questioned due to potential formatting errors and rebellious behavior from LLMs, leading to a focus on parsing model output in agent demo code.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    result[\"output_text\"]\n",
    ")  # 결과 딕셔너리에서 'output_text' 키에 해당하는 값을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which includes task decomposition and self-reflection; memory, detailing types of memory and maximum inner product search (MIPS); and tool use, illustrated through case studies such as scientific discovery agents and generative agents simulations. The article also addresses the challenges faced by these agents and provides references for further reading.\n",
      "\n",
      "The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals and self-reflection for continuous improvement; memory, detailing types of memory and maximum inner product search (MIPS); and tool use, illustrated through case studies such as scientific discovery agents and generative agents simulations. The article highlights the potential of LLMs as powerful general problem solvers, beyond their traditional applications in generating text. It also discusses challenges faced by these agents and references several proof-of-concept demos, including AutoGPT, GPT-Engineer, and BabyAGI, as inspiring examples. Additionally, it provides references for further reading.\n",
      "\n",
      "The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, which involves task decomposition into manageable subgoals and self-reflection for continuous improvement; memory, which includes short-term memory for in-context learning and long-term memory for retaining and recalling information using external vector stores; and tool use, where agents learn to call external APIs for additional information, code execution, and access to proprietary sources. The article highlights the potential of LLMs as powerful general problem solvers, extending beyond traditional text generation applications. It also discusses challenges faced by these agents and references several proof-of-concept demos, including AutoGPT, GPT-Engineer, and BabyAGI, as inspiring examples. Additionally, it provides references for further reading.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original summary remains accurate and comprehensive without needing changes. The article \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, explores the architecture and functionality of autonomous agents powered by large language models (LLMs). It outlines three key components: planning, memory, and tool use, emphasizing agents' ability to call external APIs for enhanced capabilities. The ReAct framework integrates reasoning and acting, while the Reflexion framework enhances reasoning skills through dynamic memory and self-reflection. A significant aspect of the memory component is the long-term memory module, which records agents' experiences in natural language, facilitating inter-agent communication and context retrieval. The reflection mechanism synthesizes these memories into higher-level inferences, guiding future actions and optimizing decision-making. The article highlights challenges such as hallucination, limited context length, and the difficulties in long-term planning and task decomposition, which hinder LLMs' robustness compared to human learning. Notably, while LLM evaluations suggest GPT-4 and ChemCrow perform similarly, expert evaluations reveal ChemCrow significantly outperforms GPT-4 in chemical correctness, indicating LLMs may struggle to assess their own performance in specialized domains. Additionally, the reliability of the natural language interface used by current agent systems is questioned due to potential formatting errors and rebellious behavior from LLMs, leading to a focus on parsing model output in agent demo code.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 요약 결과를 JSON 형식으로 저장합니다.\n",
    "output_data = {\n",
    "    \"question_template\": prompt_template,\n",
    "    \"reduce_template\": refine_template,\n",
    "    \"summary\": answer[\"output_text\"]\n",
    "}\n",
    "# 결과를 JSON 파일로 저장합니다.\n",
    "with open(\"Refine_result.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
